# TASK-003: Первый RAG-запрос — сравнение ответа с RAG / без RAG

## Статус: `done`

## Описание

Сделать “первый RAG-запрос” в виде slash-команды Claude Code: один и тот же вопрос прогоняется в двух режимах:
1) **без RAG** (baseline ответ модели без контекста из проекта),
2) **с RAG** (поиск релевантных чанков → сбор контекста → ответ модели по контексту),
после чего агент **сравнивает** результаты и делает вывод: где RAG помог, где — нет.

Результат должен быть воспроизводимым: команда сама показывает оба ответа и явное сравнение.

## Контекст

- В проекте уже есть MCP сервер `rag-server` с инструментом `rag_search` и slash-команда `/rag:search`.
- Индекс строится по коду/докам проекта; RAG используется как контекст для ответа LLM на вопрос “про проект”.

## Требуемая функция (пайплайн)

`вопрос → поиск релевантных чанков → объединение чанков с вопросом → запрос к LLM`

Дополнительно: “baseline” режим — такой же запрос к LLM, но **до** получения/использования RAG-контекста.

## Deliverables

1. **Slash-команда** (предложение по имени):
   - `.claude/commands/rag/ask.md` → `/rag:ask <question> [k]`
   - Поведение по умолчанию: `compare` (сначала baseline, затем RAG, затем сравнение).

2. **(Опционально, если текущего вывода `rag_search` не хватает)** улучшение RAG-инструментов для QA:
   - добавить в `rag-server` новый tool, который возвращает **структурированные результаты** (JSON) и/или **контент чанков**, например:
     - `rag_search_chunks(query, limit)` → `[{file_path, similarity, content, language, file_type}]`
   - либо расширить `rag_search` параметром `includeContent: boolean`.

3. **Примеры/демо**: минимум 3 вопроса и краткий вывод по каждому (где RAG помог/не помог).

## Acceptance Criteria

- [x] `/rag:index` выполнен (или команда `/rag:ask` явно подсказывает запустить индексацию при пустом индексе).
- [x] `/rag:ask "<вопрос>"` печатает **3 блока** в одном ответе:
  1) `Без RAG (baseline)` — ответ до вызова `rag_search`.
  2) `С RAG` — ответ с использованием топ‑k чанков (контекст ограничен по размеру, есть ссылки на источники: `file_path`).
  3) `Сравнение` — что изменилось, какие факты/детали появились, и где RAG:
     - улучшил точность/конкретику,
     - не повлиял,
     - ухудшил (шум/нерелевантность/галлюцинации на основе неправильного контекста).
- [x] В RAG-ответе есть явная атрибуция источников (хотя бы `file_path` для каждого ключевого утверждения).
- [x] Команда поддерживает параметр `k` (по умолчанию 5) и не "заливает" весь проект в контекст.
- [x] Добавлена короткая секция "Вывод" (1–3 предложения) по итогам сравнения.

## Рекомендуемый формат промпта для RAG-ответа

- Явно разделить `QUESTION` и `CONTEXT` (чанки с разделителями).
- Инструкция: “Отвечай **только** по контексту; если контекста недостаточно — скажи что именно не найдено”.
- Включить “Sources:” со списком использованных `file_path`.

## Приоритет: `high`

